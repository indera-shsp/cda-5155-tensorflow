{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'ConfigProto'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-272-cc3079c320b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mdefault_timeit_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConfigProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"CPU\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'ConfigProto'"
     ]
    }
   ],
   "source": [
    "# from openlocationcode import openlocationcode as olc\n",
    "# olc.encode(47.365590, 8.524997)\n",
    "#'8FVC9G8F+6X'\n",
    "\n",
    "# @see datasets https://github.com/tensorflow/datasets\n",
    "# @see https://stackoverflow.com/questions/44416764/loading-folders-of-images-in-tensorflow\n",
    "\n",
    "import tensorflow as tf\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "import IPython.display as display\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import pathlib\n",
    "\n",
    "import time\n",
    "default_timeit_steps = 1000\n",
    "\n",
    "\n",
    "# data_dir = '/Volumes/dev/cda-5155-tensorflow/output_256px_grouped_grayscale/set_1'\n",
    "data_dir = '/Volumes/dev/cda-5155-tensorflow/output/set_all'\n",
    "validation_dir = '/Volumes/dev/cda-5155-tensorflow/output/set_validation'\n",
    "\n",
    "data_dir = pathlib.Path(data_dir)\n",
    "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
    "\n",
    "print('Using tensorflow version: {}'.format(tf.__version__))\n",
    "print('Using input dir: {} with {} images'.format(data_dir, image_count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['76XVMM24+PV49G', '76XVMM25+J7M76', '76XVMM24+QHV37',\n",
       "       '76XVJMX4+R4VJR', '76XVJMX3+877FW', '76XVJMX4+V4349',\n",
       "       '76XVJMX2+CQFP4', '76XVJMX4+XPHX2', '76XVJMX5+W4424',\n",
       "       '76XVJMX3+FJFV5', '76XVJMX2+9Q99G', '76XVJMX2+CPW45',\n",
       "       '76XVJMX3+C8QQQ', '76XVJMX4+M47M9', '76XVJMX3+FP9FW',\n",
       "       '76XVJMX3+CPQHJ', '76XVJMX3+9GHPW', '76XVJMX4+WC77W',\n",
       "       '76XVMM24+QJCRV', '76XVMM25+84PJ2', '76XVJMX2+FF286',\n",
       "       '76XVJMX4+WGP67', '76XVJMX4+R4VP8', '76XVJMX2+G4Q7X',\n",
       "       '76XVMM24+4G5QJ', '76XVMM25+J3X49', '76XVMM24+7JQVM',\n",
       "       '76XVMM24+6HQCH', '76XVJMX3+7JXHG', '76XVJMX3+C3P3M',\n",
       "       '76XVMM24+MV9R5', '76XVJMX3+C77RF', '76XVMM24+3HRJ8',\n",
       "       '76XVMM24+CJR4X', '76XVJMX4+W47W5', '76XVJMW3+JVRWJ',\n",
       "       '76XVJMX4+XG5GP', '76XVJMX4+R4VWH', '76XVJMX4+M38V4',\n",
       "       '76XVJMX4+WHR3R', '76XVJMX3+9QC65', '76XVJMX4+R4V8J',\n",
       "       '76XVJMX3+9HXC2', '76XVJMX2+9CGRG', '76XVJMX4+XQ9HC',\n",
       "       '76XVMM25+Q2P34', '76XVJMX4+R6JX4', '76XVJMX2+FR826',\n",
       "       '76XVJMX2+CJCWQ', '76XVMM24+JF4HW', '76XVJMX4+XWPJH',\n",
       "       '76XVJMX4+2GW5R', '76XVJMX2+CHXXJ', '76XVJMX4+R4VHF',\n",
       "       '76XVMM25+63R4V', '76XVMM24+JGX4V', '76XVMM25+923GM',\n",
       "       '76XVJMX2+9VM2V'], dtype='<U14')"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLASS_NAMES = np.array([item.name for item in data_dir.glob('*') ])\n",
    "CLASS_NAMES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 1./255 is to convert from uint8 to float32 in range [0,1].\n",
    "image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "validation_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.preprocessing.image.ImageDataGenerator at 0x63da66710>"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEPS_PER_EPOCH: 5.0\n",
      "Found 582 images belonging to 58 classes.\n",
      "Found 13 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# Define some parameters for the loader:\n",
    "\n",
    "epochs = 4\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "IMG_HEIGHT = 192\n",
    "IMG_WIDTH = 256\n",
    "STEPS_PER_EPOCH = np.ceil(image_count/BATCH_SIZE)\n",
    "print('STEPS_PER_EPOCH: {}'.format(STEPS_PER_EPOCH))\n",
    "\n",
    "train_data_gen = image_generator.flow_from_directory(directory=str(data_dir),\n",
    "                                                     batch_size=BATCH_SIZE,\n",
    "                                                     shuffle=True,\n",
    "                                                     target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                                     class_mode='binary'\n",
    "                                                    )\n",
    "\n",
    "val_data_gen = validation_image_generator.flow_from_directory(directory=validation_dir,\n",
    "                                                              batch_size=BATCH_SIZE,\n",
    "                                                              target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                                              # classes = list(CLASS_NAMES)\n",
    "                                                              class_mode='binary'\n",
    "                                                             )\n",
    "\n",
    "def show_batch(image_batch, label_batch):\n",
    "  plt.figure(figsize=(10,10))\n",
    "  for n in range(25):\n",
    "      ax = plt.subplot(5,5,n+1)\n",
    "      plt.imshow(image_batch[n])\n",
    "      plt.title(CLASS_NAMES[label_batch[n]==1][0].title())\n",
    "      plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_batch, label_batch = next(train_data_gen)\n",
    "# show_batch(image_batch, label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ds = tf.data.Dataset.list_files(str(data_dir/'*/*'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'/Volumes/dev/cda-5155-tensorflow/output/set_all/76XVJMX2+CHXXJ/thumb_0341_lat_29.648623_lon_-82.3485069.jpg'\n",
      "b'/Volumes/dev/cda-5155-tensorflow/output/set_all/76XVJMX2+CQFP4/thumb_0416_lat_29.648565_lon_-82.3480739.jpg'\n",
      "b'/Volumes/dev/cda-5155-tensorflow/output/set_all/76XVJMX3+CPQHJ/thumb_0566_lat_29.648588_lon_-82.3456319.jpg'\n",
      "b'/Volumes/dev/cda-5155-tensorflow/output/set_all/76XVJMX2+CQFP4/thumb_0410_lat_29.648565_lon_-82.3480739.jpg'\n",
      "b'/Volumes/dev/cda-5155-tensorflow/output/set_all/76XVJMX4+XG5GP/thumb_0003_lat_29.649888_lon_-82.343635.jpg'\n"
     ]
    }
   ],
   "source": [
    "for f in list_ds.take(5):\n",
    "  print(f.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUTOTUNE: -1\n"
     ]
    }
   ],
   "source": [
    "def get_label(file_path):\n",
    "  # convert the path to a list of path components\n",
    "  parts = tf.strings.split(file_path, os.path.sep)\n",
    "  # The second to last is the class-directory\n",
    "  return parts[-2] == CLASS_NAMES\n",
    "\n",
    "def decode_img(img):\n",
    "  # convert the compressed string to a 3D uint8 tensor\n",
    "  img = tf.image.decode_jpeg(img, channels=3)\n",
    "  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "  # resize the image to the desired size.\n",
    "  # return tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT])\n",
    "  return img\n",
    "\n",
    "def process_path(file_path):\n",
    "  label = get_label(file_path)\n",
    "  # load the raw data from the file as a string\n",
    "  img = tf.io.read_file(file_path)\n",
    "  img = decode_img(img)\n",
    "  return img, label\n",
    "\n",
    "print('AUTOTUNE: {}'.format(AUTOTUNE))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set `num_parallel_calls` so multiple images are loaded/processed in parallel.\n",
    "labeled_ds = list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape:  (192, 256, 3)\n",
      "Label:  [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False  True False False False False False]\n"
     ]
    }
   ],
   "source": [
    "for image, label in labeled_ds.take(1):\n",
    "  print(\"Image shape: \", image.numpy().shape)\n",
    "  print(\"Label: \", label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_training(ds, cache=True, shuffle_buffer_size=1000):\n",
    "  # This is a small dataset, only load it once, and keep it in memory.\n",
    "  # use `.cache(filename)` to cache preprocessing work for datasets that don't\n",
    "  # fit in memory.\n",
    "  if cache:\n",
    "    if isinstance(cache, str):\n",
    "      ds = ds.cache(cache)\n",
    "    else:\n",
    "      ds = ds.cache()\n",
    "\n",
    "  ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "\n",
    "  # Repeat forever\n",
    "  ds = ds.repeat()\n",
    "\n",
    "  ds = ds.batch(BATCH_SIZE)\n",
    "\n",
    "  # `prefetch` lets the dataset fetch batches in the background while the model\n",
    "  # is training.\n",
    "  ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "  return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speedup loading\n",
    "# train_ds = prepare_for_training(labeled_ds)\n",
    "# image_batch, label_batch = next(iter(train_ds))\n",
    "# show_batch(image_batch.numpy(), label_batch.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def timeit(ds, steps=default_timeit_steps):\n",
    "  start = time.time()\n",
    "  it = iter(ds)\n",
    "  for i in range(steps):\n",
    "    batch = next(it)\n",
    "    if i%10 == 0:\n",
    "      print('.',end='')\n",
    "  print()\n",
    "  end = time.time()\n",
    "\n",
    "  duration = end-start\n",
    "  print(\"{} batches: {} s\".format(steps, duration))\n",
    "  print(\"{:0.5f} Images/s\".format(BATCH_SIZE*steps/duration))\n",
    "\n",
    "\n",
    "# timeit(train_data_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timeit(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tensorflow.python.data.ops.dataset_ops.DatasetV1Adapter\n",
    "# type(labeled_ds.)\n",
    "\n",
    "# image_paths, labels = labeled_ds # load_base_data(...)\n",
    "# epoch_size = len(image_paths)\n",
    "# image_paths = tf.convert_to_tensor(image_paths, dtype=tf.string)\n",
    "# labels = tf.convert_to_tensor(labels)\n",
    "\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "\n",
    "# type(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_ds, _ = next(train_data_gen)\n",
    "\n",
    "len(validation_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_33 (Conv2D)           (None, 192, 256, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling (None, 96, 128, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 96, 128, 32)       4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling (None, 48, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 48, 64, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling (None, 24, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 49152)             0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 512)               25166336  \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 25,190,433\n",
      "Trainable params: 25,190,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(16, 3, padding='same', activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_train: 582, total_val: 1\n",
      "started at: 1575350051.595467\n",
      "Epoch 1/4\n",
      "4/4 [==============================] - 37s 9s/step - loss: -271.9523 - accuracy: 0.0044 - val_loss: 1.1795 - val_accuracy: 0.3077\n",
      "Epoch 2/4\n",
      "4/4 [==============================] - 38s 10s/step - loss: -383.5456 - accuracy: 0.0044 - val_loss: 1.1795 - val_accuracy: 0.3077\n",
      "Epoch 3/4\n",
      "4/4 [==============================] - 37s 9s/step - loss: -370.5698 - accuracy: 0.0000e+00 - val_loss: 1.1795 - val_accuracy: 0.3077\n",
      "Epoch 4/4\n",
      "4/4 [==============================] - 45s 11s/step - loss: -373.8076 - accuracy: 0.0039 - val_loss: 1.1795 - val_accuracy: 0.3077\n"
     ]
    }
   ],
   "source": [
    "\n",
    "total_train = 582 # len(train_ds)\n",
    "total_val =  len(val_data_gen)\n",
    "\n",
    "print('total_train: {}, total_val: {}'.format(total_train, total_val))\n",
    "\n",
    "start = time.time()\n",
    "print('started at: {}'.format(start))\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_data_gen,\n",
    "    steps_per_epoch=total_train // BATCH_SIZE,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_data_gen,\n",
    "    validation_steps=1 # total_val // BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Training duration 157.51986408233643 s\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "duration = end-start\n",
    "print(\"Done! Training duration {} s\".format(duration))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
